{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pinecone\n",
    "\n",
    "The goal of this colab is to use the vector database pinecone to create embeddings of new documents, and store them, create a doc searcher, and use that to talk to documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install libraries\n",
    "This version uses llama_hubs vectore store and default query engine modules.\n",
    "the underlying model that is called is openai's gpt3.5 model but can be changed to anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index==\"0.8.41\"\n",
    "%pip install llama_hub==\"0.0.36\"\n",
    "%pip install youtube_transcript_api==\"0.6.1\"\n",
    "%pip install pinecone-client==\"2.2.4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Pinecone Account and Index\n",
    "\n",
    "### To proceed we need some initial steps for pinecone:\n",
    "\n",
    "- Register an account at pinecone.io\n",
    "- Configure and create an index according to your [needs](https://docs.pinecone.io/docs/choosing-index-type-and-size#overview) \n",
    "\n",
    "For this demo I have used my account with the starter plan which should be sufficient for the amount of documents we need to index\n",
    "\n",
    "![](images/pinecone-create-index.png)\n",
    "\n",
    "- finally wait for the index to be initialized and be ready\n",
    "\n",
    "![](images/pinecone-index-view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration of modules\n",
    "\n",
    "- logging and required api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import getpass\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARN)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download youtube video's transcripts\n",
    "\n",
    "- using llama_hub's [youtube transcript loader] (https://llamahub.ai/l/youtube_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=VH8lEOmesas','https://www.youtube.com/watch?v=ZSEnQ00iUgI'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index in pinecone with embeddings\n",
    "\n",
    "- using llama_hubs vectorstore wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "\n",
    "pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "pinecone.init(api_key=pinecone_api_key, environment=\"gcp-starter\")\n",
    "pinecone_index = pinecone.Index(\"playground\")\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=StorageContext.from_defaults(vector_store=vector_store)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use query engine to talk to docs\n",
    "\n",
    "this uses the vector store wrapper from [llama_index](https://gpt-index.readthedocs.io/en/v0.6.5/how_to/integrations/vector_stores.html)\n",
    "\n",
    "note: indexing might take a while (a couple of seconds) on pinecone.io (since its cloud based and its starter plan) and you should wait a bit before firing the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can contact the Deep Shore Cloud Archive with Microservices for assistance\n",
      "with running old software and services in your company. They offer a cloud-\n",
      "based archiving solution that is highly available, always-on, and freely\n",
      "scalable. Their solution is designed to provide unlimited access to company\n",
      "data, transactions, and documents, while ensuring full regulatory compliance\n",
      "and security. Additionally, they offer blockchain-based cloud solutions that\n",
      "can revolutionize your digital archiving by providing incorruptible bookkeeping\n",
      "of digital transactions.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"I still run old Softare and Services in my company. Who you gonna call?\"         \n",
    "response = query_engine.query(question)\n",
    "\n",
    "wrapped_lines = textwrap.wrap(response.response, 79, break_long_words=False)\n",
    "\n",
    "for line in wrapped_lines:\n",
    "  print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-5min",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
