{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Chroma\n",
    "\n",
    "The goal of this colab is to have a vector database like chroma to create embeddings of new documents, and store them, create a doc searcher, and use that to talk to documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install libraries\n",
    "This version uses llama_hubs vectore store and default query engine modules.\n",
    "the underlying model that is called is openai's gpt3.5 model but can be changed to anything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_index==\"0.8.41\"\n",
    "%pip install llama_hub==\"0.0.36\"\n",
    "%pip install chromadb==\"0.4.15\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configuration of modules\n",
    "\n",
    "- logging and required openai api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import getpass\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARN)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download youtube video's transcripts\n",
    "\n",
    "- using llama_hub's [youtube transcript loader] (https://llamahub.ai/l/youtube_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=VH8lEOmesas','https://www.youtube.com/watch?v=ZSEnQ00iUgI'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index in chroma with embeddings\n",
    "\n",
    "- using llama_hubs vectorstore wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "chromadb_settings = Settings(anonymized_telemetry=False)\n",
    "client = chromadb.Client(settings=chromadb_settings)\n",
    "collection = client.get_or_create_collection(name=\"test\")\n",
    "chromastore = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=StorageContext.from_defaults(vector_store=chromastore)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use query engine to talk to docs\n",
    "\n",
    "this uses the vector store wrapper from [llama_index](https://gpt-index.readthedocs.io/en/v0.6.5/how_to/integrations/vector_stores.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should contact the Deep Shore Cloud Archive with microservices. They have\n",
      "developed a solution that can turn your traditional archive into a real-time\n",
      "enabled Enterprise Information Catalog. Their services offer unlimited access\n",
      "from anywhere in the world, full regulatory compliance and security, and\n",
      "scalability in bandwidth and data volume. They can also feed ERP systems and\n",
      "data warehouses with the complete digital knowledge of your business in real\n",
      "time.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "question = \"I still run old Softare and Services in my company. Who you gonna call?\"         \n",
    "response = query_engine.query(question)\n",
    "\n",
    "wrapped_lines = textwrap.wrap(response.response, 79, break_long_words=False)\n",
    "\n",
    "for line in wrapped_lines:\n",
    "  print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-5min",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
